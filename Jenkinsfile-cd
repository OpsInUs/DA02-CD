pipeline {
    agent any // Ensure agent has gcloud, git, helm, kubectl, gke-gcloud-auth-plugin

    parameters {
        string(name: 'ADMIN_SERVER_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-admin-server')
        string(name: 'API_GATEWAY_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-api-gateway')
        string(name: 'CONFIG_SERVER_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-config-server')
        string(name: 'CUSTOMERS_SERVICE_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-customers-service')
        string(name: 'DISCOVERY_SERVER_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-discovery-server')
        string(name: 'GENAI_SERVICE_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-genai-service')
        string(name: 'VETS_SERVICE_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-vets-service')
        string(name: 'VISITS_SERVICE_BRANCH', defaultValue: 'main', description: 'Branch for spring-petclinic-visits-service')
    }

    environment {
        HELM_RELEASE_NAME         = 'petclinic'
        HELM_CHART_REPO_DIR_NAME  = 'DA02-HelmRepo' // Directory where the Helm chart Git repo is cloned
        HELM_REPO_HTTPS_URL       = 'https://github.com/OpsInUs/DA02-HelmRepo.git'
        // *** IMPORTANT: ADJUST THIS PATH IF YOUR CHART IS IN A SUBDIRECTORY OF THE REPO ***
        // This assumes your Helm chart named 'petclinic' is in a directory 'petclinic'
        // at the root of the HELM_CHART_REPO_DIR_NAME checkout.
        // If the chart itself is the root of HELM_CHART_REPO_DIR_NAME, then LOCAL_CHART_PATH should be HELM_CHART_REPO_DIR_NAME
        LOCAL_CHART_PATH          = "${env.HELM_CHART_REPO_DIR_NAME}" 

        SERVICES_CD_REPO_URL      = 'https://github.com/OpsInUs/DA02-CD.git' // Repo for service source code to get commit IDs
        DOCKER_REGISTRY_USERNAME  = 'vuhoabinhthachhoa'
        TARGET_NAMESPACE          = 'dev-review'

        // GKE Cluster details
        GKE_CLUSTER_NAME          = 'petclinic' // YOUR GKE CLUSTER NAME
        GKE_CLUSTER_LOCATION      = 'us-central1-a' // YOUR GKE CLUSTER ZONE/REGION

        // GCP WIF details
        GCP_PROJECT_ID            = 'thanh-devops'
        GCP_PROJECT_NUMBER        = '321371933596'
        WIF_POOL_ID               = 'jenkins-oidc-pool'
        WIF_PROVIDER_ID           = 'jenkins-provider'
        GCP_SERVICE_ACCOUNT       = 'jenkins-wif-executor-sa@thanh-devops.iam.gserviceaccount.com'
        JENKINS_OIDC_CRED_ID      = 'jenkins-gcp-oidc-token'
        GCP_WIF_AUDIENCE          = "//iam.googleapis.com/projects/${GCP_PROJECT_NUMBER}/locations/global/workloadIdentityPools/${WIF_POOL_ID}/providers/${WIF_PROVIDER_ID}"
    }

    stages {
        stage('Clone Helm Chart Repository') {
            steps {
                echo "Cleaning up previous checkout of ${HELM_CHART_REPO_DIR_NAME}..."
                dir(HELM_CHART_REPO_DIR_NAME) {
                    deleteDir()
                }
                echo "Cloning Helm chart from ${HELM_REPO_HTTPS_URL} into ./${HELM_CHART_REPO_DIR_NAME}"
                checkout([
                    $class: 'GitSCM',
                    branches: [[name: '*/main']],
                    userRemoteConfigs: [[url: HELM_REPO_HTTPS_URL]],
                    extensions: [[$class: 'RelativeTargetDirectory', relativeTargetDir: HELM_CHART_REPO_DIR_NAME]]
                ])
                script {
                    sh "echo 'Contents of ./${HELM_CHART_REPO_DIR_NAME}:'; ls -la ./${HELM_CHART_REPO_DIR_NAME}"
                    sh "echo 'Verifying chart at ./${LOCAL_CHART_PATH}'"
                    sh "if [ -d './${LOCAL_CHART_PATH}' ] && [ -f './${LOCAL_CHART_PATH}/Chart.yaml' ]; then echo 'Chart found at ./${LOCAL_CHART_PATH}'; else echo 'ERROR: Chart not found at ./${LOCAL_CHART_PATH}. Adjust LOCAL_CHART_PATH and Git repo structure.'; exit 1; fi"
                }
            }
        }

        stage('Deploy/Upgrade Petclinic with Helm') {
            steps {
                script {
                    // KUBECONFIG should be set from the previous stage
                    if (!env.KUBECONFIG || !fileExists(env.KUBECONFIG)) {
                        error "KUBECONFIG is not set or file does not exist. Authentication stage might have failed."
                    }
                    echo "Using KUBECONFIG: ${env.KUBECONFIG} for Helm deployment."

                    try {
                        def servicesToUpdate = [
                            'admin-server': params.ADMIN_SERVER_BRANCH,
                            'api-gateway': params.API_GATEWAY_BRANCH,
                            'config-server': params.CONFIG_SERVER_BRANCH,
                            'customers-service': params.CUSTOMERS_SERVICE_BRANCH,
                            'discovery-server': params.DISCOVERY_SERVER_BRANCH,
                            'genai-service': params.GENAI_SERVICE_BRANCH,
                            'vets-service': params.VETS_SERVICE_BRANCH,
                            'visits-service': params.VISITS_SERVICE_BRANCH
                        ]
                        def helmSetArgs = []
                        def servicesWithNodePort = [] // Keep track of services set to NodePort

                        servicesToUpdate.each { serviceName, branch ->
                            if (branch != 'main' && branch.trim() != '') {
                                echo "Processing ${serviceName} with branch ${branch}"
                                
                                // Get commit hash from the SERVICES_CD_REPO_URL
                                def commitHashCmd = "git ls-remote ${SERVICES_CD_REPO_URL} refs/heads/${branch} | head -1 | cut -f1"
                                def commitHash = sh(script: commitHashCmd, returnStdout: true).trim()

                                if (!commitHash) {
                                    error "Failed to get commit hash for branch '${branch}' of service '${serviceName}' from ${SERVICES_CD_REPO_URL}."
                                }
                                def tag = commitHash.substring(0, 7)
                                def imageName = "spring-petclinic-${serviceName}" // Assuming image name convention
                                
                                helmSetArgs.add("--set services.\"${serviceName}\".image.repository=${DOCKER_REGISTRY_USERNAME}/${imageName}")
                                helmSetArgs.add("--set services.\"${serviceName}\".image.tag=${tag}")
                                helmSetArgs.add("--set services.\"${serviceName}\".serviceType=NodePort") // Set to NodePort if branch isn't main
                                servicesWithNodePort.add(serviceName)
                            } else {
                                echo "Skipping image override for ${serviceName} as branch is 'main' or empty. It will use chart defaults or values from previous deployments for serviceType."
                                // If you want to ensure it's ClusterIP if main, you'd add:
                                // helmSetArgs.add("--set services.\"${serviceName}\".serviceType=ClusterIP")
                            }
                        }
                        
                        def chartPath = "./${LOCAL_CHART_PATH}" // Relative to workspace
                        helmSetArgs.add("--set global.namespace=${TARGET_NAMESPACE}") // Always set the target namespace globally
                        
                        if (helmSetArgs.size() == 1 && helmSetArgs[0] == "--set global.namespace=${TARGET_NAMESPACE}") {
                             echo "No service-specific branches provided (or all were 'main'). Deploying with chart defaults for images and service types, but targeting namespace ${TARGET_NAMESPACE}."
                        }
                        
                        // Run helm dependency update in the chart's directory
                        dir(chartPath) {
                            sh "helm dependency update"
                        }

                        def helmCommand = "helm upgrade --install ${HELM_RELEASE_NAME} ${chartPath} --namespace ${TARGET_NAMESPACE} --debug --wait"
                        if (!helmSetArgs.isEmpty()) {
                            helmCommand += " " + helmSetArgs.join(" ")
                        }
                        
                        sh helmCommand

                        // Verification steps
                        echo "Deployment for ${HELM_RELEASE_NAME} completed. Verifying status..."
                        sh 'sleep 30' // Allow some time for resources to settle
                        sh "helm status ${HELM_RELEASE_NAME} -n ${TARGET_NAMESPACE}"
                        sh "kubectl get deployments -n ${TARGET_NAMESPACE} -l app.kubernetes.io/instance=${HELM_RELEASE_NAME}"
                        sh "kubectl get services -n ${TARGET_NAMESPACE} -l app.kubernetes.io/instance=${HELM_RELEASE_NAME}"
                        sh "kubectl get pods -n ${TARGET_NAMESPACE} -l app.kubernetes.io/instance=${HELM_RELEASE_NAME}"

                        if (!servicesWithNodePort.isEmpty()) {
                            echo "Services deployed with NodePort (due to non-main branch selection): ${servicesWithNodePort.join(', ')}"
                            servicesWithNodePort.each { service ->
                                def serviceK8sName = service // Assuming Helm chart service name matches
                                // Attempt to get any TCP port's nodePort, more robust if specific port isn't always first
                                def nodePortJsonPath = "{.spec.ports[?(@.protocol=='TCP')].nodePort}"

                                def nodePortOutput = sh(script: "kubectl get service ${serviceK8sName} -n ${TARGET_NAMESPACE} -o jsonpath='${nodePortJsonPath}'", returnStdout: true).trim()
                                def nodePort = nodePortOutput.split(' ')[0] // Take the first nodePort if multiple are returned

                                if (nodePort) {
                                    def nodeIp = sh(script: "kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"ExternalIP\")].address}'", returnStdout: true).trim()
                                    if (!nodeIp) {
                                        nodeIp = sh(script: "kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"InternalIP\")].address}'", returnStdout: true).trim()
                                    }
                                    if (nodeIp) {
                                        echo "Service: ${serviceK8sName} - NodePort Endpoint: http://${nodeIp}:${nodePort}"
                                    } else {
                                        echo "Service: ${serviceK8sName} - NodePort ${nodePort}. Could not determine GKE Node External/Internal IP."
                                    }
                                } else {
                                    echo "Service: ${serviceK8sName} - Failed to get NodePort. Ensure it's exposed as NodePort in the chart and cluster allows NodePorts."
                                }
                            }
                        }

                    } catch (Exception e) {
                        echo "Error during Helm deploy/upgrade for ${HELM_RELEASE_NAME}: ${e.getMessage()}"
                        sh "helm status ${HELM_RELEASE_NAME} -n ${TARGET_NAMESPACE} || echo 'Helm status failed or release not found.'"
                        sh "kubectl get events -n ${TARGET_NAMESPACE} --sort-by=.metadata.creationTimestamp | tail -n 30 || echo 'Failed to get k8s events.'"
                        error "Failed to deploy/upgrade ${HELM_RELEASE_NAME} application."
                    }
                }
            }
        }
    } // End stages

    post {
        always {
            script {
                echo "Post-build cleanup..."
                dir(HELM_CHART_REPO_DIR_NAME) {
                    echo "Cleaning up ./${HELM_CHART_REPO_DIR_NAME}"
                    deleteDir()
                }

                // WIF and gcloud temp files cleanup
                if (env.WIF_CONFIG_FILE && fileExists(env.WIF_CONFIG_FILE)) {
                    echo "Cleaning up WIF config file: ${env.WIF_CONFIG_FILE}"
                    sh "rm -f ${env.WIF_CONFIG_FILE}"
                }
                if (env.TEMP_GCLOUD_CONFIG_DIR && fileExists(env.TEMP_GCLOUD_CONFIG_DIR)) {
                    echo "Cleaning up gcloud temp config dir: ${env.TEMP_GCLOUD_CONFIG_DIR}"
                    sh "rm -rf ${env.TEMP_GCLOUD_CONFIG_DIR}"
                }
                if (env.TEMP_KUBECONFIG_FILE && fileExists(env.TEMP_KUBECONFIG_FILE)) {
                    echo "Cleaning up temp kubeconfig: ${env.TEMP_KUBECONFIG_FILE}"
                    sh "rm -f ${env.TEMP_KUBECONFIG_FILE}"
                }
            }
        }
    }
}
